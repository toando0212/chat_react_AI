import re
# Lo·∫°i b·ªè reasoning trace/th·∫ª <think> kh·ªèi ph·∫£n h·ªìi
def remove_think_tags(text):
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
import streamlit as st
from query import *
from functools import lru_cache
import argparse
import requests
import importlib.util
from pymongo import MongoClient
import google.generativeai as genai

import os
from dotenv import load_dotenv
load_dotenv()


def get_secret(key):
    # ∆Øu ti√™n l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng (local .env)
    value = os.getenv(key)
    if value is not None:
        return value
    # N·∫øu kh√¥ng c√≥, th·ª≠ l·∫•y t·ª´ st.secrets (Streamlit Cloud)
    try:
        if hasattr(st, "secrets") and key in st.secrets:
            return st.secrets[key]
    except Exception:
        pass
    # N·∫øu kh√¥ng c√≥, raise exception r√µ r√†ng
    raise RuntimeError(f"Secret '{key}' not found in environment variables or st.secrets!")


GEMINI_API_KEY = get_secret("GEMINI_API_KEY")
genai.configure(api_key=GEMINI_API_KEY)

# Cache MongoDB client globally (Streamlit Cloud safe)
@st.cache_resource
def get_mongodb_client():
    MONGODB_URI = get_secret("MONGODB_URI")
    try:
        client = MongoClient(
            MONGODB_URI,
            maxPoolSize=10,
            serverSelectionTimeoutMS=5000,
            connectTimeoutMS=5000,
            socketTimeoutMS=5000
        )
        # Try a quick server_info to force connection
        client.server_info()
        return client
    except Exception as e:
        st.error(f"‚ùå L·ªói k·∫øt n·ªëi MongoDB: {e}")
        raise

# Optionally cache embedding generation
@lru_cache(maxsize=1000)
def get_embedding_cached(text):
    return get_embedding(text)

def build_context(docs):
    context = ""
    for i, doc in enumerate(docs, 1):
        context += f"[Doc {i}]\nGi·∫£i th√≠ch: {doc.get('explanation')}\nCode: {doc.get('code')}\nLink: {doc.get('link')}\n\n"
    return context


# H√†m g·ªçi Cerebras API v·ªõi chat history ƒë·ªÉ duy tr√¨ cu·ªôc h·ªôi tho·∫°i
def ask_cerebras(question, context, chat_history=None, model="llama-4-scout-17b-16e-instruct"):
    '''
    H√†m g·ªçi Cerebras API v·ªõi chat history ƒë·ªÉ duy tr√¨ cu·ªôc h·ªôi tho·∫°i
    chat_history: list of {"role": "user/assistant", "content": "..."}
    '''
    # Import cerebras lib n·∫øu c√≥, n·∫øu kh√¥ng b√°o l·ªói r√µ r√†ng
    try:
        spec = importlib.util.find_spec("cerebras.cloud.sdk")
        if spec is None:
            raise ImportError("B·∫°n c·∫ßn c√†i ƒë·∫∑t th∆∞ vi·ªán cerebras-cloud-sdk: pip install cerebras-cloud-sdk")
        from cerebras.cloud.sdk import Cerebras
    except ImportError as e:
        return f"‚ùå L·ªói: {e}"

    api_key = get_secret("CEREBRAS_API_KEY")
    # Load system prompt
    with open("prompt.txt", "r", encoding="utf-8") as f:
        system_prompt = f.read().strip()
    # T·∫°o messages v·ªõi system prompt + chat history + context
    messages = [{"role": "system", "content": system_prompt}]
    if chat_history:
        messages.extend(chat_history)
    current_message = f"Context:\n{context}\n\nQuestion: {question}"
    messages.append({"role": "user", "content": current_message})
    try:
        client = Cerebras(api_key=api_key)
        response = client.chat.completions.create(
            messages=messages,
            model=model,
            temperature=0.2,
            max_tokens=1024
        )
        # Tr·∫£ v·ªÅ n·ªôi dung tr·∫£ l·ªùi
        return response.choices[0].message.content
    except Exception as e:
        return f"‚ùå L·ªói g·ªçi Cerebras API: {e}"

def get_chatbot_response(question, chat_history=None, topk=5, model="gpt-oss-120b"):
    '''
    H√†m ch√≠nh ƒë·ªÉ l·∫•y ph·∫£n h·ªìi t·ª´ chatbot v·ªõi chat history
    Returns: (answer, context_info, updated_chat_history)
    '''
    # D√πng MongoDB client ƒë√£ cache
    client = get_mongodb_client()
    db = client["chatcodeai"]
    collection = db["normalized"]

    # T√¨m ki·∫øm t√†i li·ªáu li√™n quan nh·∫•t b·∫±ng vector embedding (c√≥ cache)
    query_emb = get_embedding_cached(question)
    query_emb = resize_embedding(query_emb, 1024)
    docs = find_top_k(query_emb, collection, k=topk)

    # N·∫øu kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o
    if not docs:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan.", "", chat_history
    # X√¢y d·ª±ng ng·ªØ c·∫£nh t·ª´ t√†i li·ªáu - docs l√† list c√°c dict
    context = build_context(docs)
    # G·ªçi Cerebras ƒë·ªÉ l·∫•y ph·∫£n h·ªìi
    answer = ask_cerebras(question, context, chat_history, model)
    # L∆∞u l·ªãch s·ª≠ tr√≤ chuy·ªán
    if chat_history is None:
        chat_history = []
    chat_history.append({"role": "user", "content": question})
    chat_history.append({"role": "assistant", "content": answer})
    # Tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi, ng·ªØ c·∫£nh v√† l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë√£ c·∫≠p nh·∫≠t
    return answer, context, chat_history

def main():
    parser = argparse.ArgumentParser(description="Chatbot embedding + Cerebras")
    parser.add_argument('--question', type=str, help='C√¢u h·ªèi ƒë·∫ßu v√†o')
    args = parser.parse_args()

    chat_history = []

    if args.question:
        # Single question mode
        answer, context_info, _ = get_chatbot_response(args.question, chat_history)
        answer = remove_think_tags(answer)
        print(answer)
    else:
        # Interactive chat mode
        print("ü§ñ Ch√†o b·∫°n! T√¥i l√† chatbot ReactJS (Cerebras). G√µ 'quit' ƒë·ªÉ tho√°t.")
        while True:
            question = input("\nüë§ B·∫°n: ").strip()
            if question.lower() in ['quit', 'exit', 'q']:
                print("üëã T·∫°m bi·ªát!")
                break
            if not question:
                continue

            print("üîç ƒêang t√¨m ki·∫øm v√† x·ª≠ l√Ω...")
            answer, context_info, chat_history = get_chatbot_response(question, chat_history, args.topk, args.model)
            answer = remove_think_tags(answer)
            print(f"\nü§ñ Bot: {answer}")

if __name__ == "__main__":
    main()
